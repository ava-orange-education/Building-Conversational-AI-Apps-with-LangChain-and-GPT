{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a8cb5b",
      "metadata": {
        "id": "86a8cb5b",
        "outputId": "bab03eb3-d84d-4c60-d823-e91a1ba5a92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\windows 10\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk==3.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sGepDp3xo46K",
      "metadata": {
        "id": "sGepDp3xo46K"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf285c5",
      "metadata": {
        "id": "5bf285c5"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9313ef3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9313ef3",
        "outputId": "7bbc4f9e-f075-492e-a986-2afb69b64f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'symbols', ',', 'known', 'as', 'tokens', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "# Sample text for tokenization\n",
        "text = \"Tokenization is the process of breaking text into smaller units, such as words, phrases, or symbols, known as tokens.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2086b4",
      "metadata": {
        "id": "da2086b4"
      },
      "source": [
        "### Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5790e4e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5790e4e5",
        "outputId": "68348109-edb1-4aa0-9be3-4b0ce60ebd7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "NLTK is a leading platform for building Python programs to work with human language data.\n",
            "\n",
            "Text after stopword removal:\n",
            "NLTK leading platform building Python programs work human language data .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Join filtered tokens back into a sentence\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\nText after stopword removal:\")\n",
        "print(filtered_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fa8ab5",
      "metadata": {
        "id": "b2fa8ab5"
      },
      "source": [
        "### NLP Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ca7ccf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ca7ccf",
        "outputId": "d7da97f6-f069-42dc-8b5d-114bc4349d8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: This is a sample text, with punctuation and  extra  spaces !\n",
            "Normalized text: this is a sample text with punctuation and extra spaces\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "original_text = \"This is a sample text, with punctuation and  extra  spaces !\"\n",
        "normalized_text = normalize_text(original_text)\n",
        "print(\"Original text:\", original_text)\n",
        "print(\"Normalized text:\", normalized_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fb2269",
      "metadata": {
        "id": "b6fb2269"
      },
      "source": [
        "### Rule-based POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12467422",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12467422",
        "outputId": "5cc17cb6-9a38-451f-b19b-3c0d978eb36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('The', 'DET'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NOUN'), ('over', 'NN'), ('the', 'DET'), ('lazy', 'NN'), ('dog', 'NN'), ('.', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "tagged_words = []\n",
        "for word in tokens:\n",
        "    if word.lower() in ['the', 'a', 'an']:\n",
        "        tagged_words.append((word, 'DET'))\n",
        "    elif word.endswith('s'):\n",
        "        tagged_words.append((word, 'NOUN'))\n",
        "    else:\n",
        "        tagged_words.append((word, 'NN'))\n",
        "\n",
        "print(tagged_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb5799a",
      "metadata": {
        "id": "3cb5799a"
      },
      "source": [
        "### Probabilistic POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee554b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee554b94",
        "outputId": "d33c89af-f963-4364-bc45-f01b916d7f19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "tagged_words = nltk.pos_tag(tokens)\n",
        "print(tagged_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e418e8b",
      "metadata": {
        "id": "1e418e8b"
      },
      "source": [
        "### Rule-based NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbdf9d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fbdf9d5",
        "outputId": "612e5de1-ce4d-4e71-fea5-c498c7f48148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Apple', 'ORGANIZATION'), ('Inc', 'ORGANIZATION'), ('Cupertino', 'ORGANIZATION'), ('California', 'ORGANIZATION')]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def rule_based_ner(text):\n",
        "    entities = []\n",
        "    for match in re.finditer(r'\\b[A-Z][a-z]*\\b', text):\n",
        "        entities.append((match.group(0), 'ORGANIZATION'))\n",
        "    return entities\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "entities = rule_based_ner(text)\n",
        "print(entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d253798f",
      "metadata": {
        "id": "d253798f"
      },
      "source": [
        "### Machine Learning-based NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64b3f58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b64b3f58",
        "outputId": "f6d572b4-061f-4494-faf7-d434fdbe0960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  is/VBZ\n",
            "  headquartered/VBN\n",
            "  in/IN\n",
            "  (GPE Cupertino/NNP)\n",
            "  ,/,\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def ml_based_ner(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged_words = nltk.pos_tag(tokens)\n",
        "    entities = nltk.chunk.ne_chunk(tagged_words)\n",
        "    return entities\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "entities = ml_based_ner(text)\n",
        "print(entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad981d1",
      "metadata": {
        "id": "0ad981d1"
      },
      "source": [
        "### NER evaluation metrics and challenges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221306ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "221306ce",
        "outputId": "16c3c4f3-9b05-41da-8681-29b363a65e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Example NER system outputs and ground truth labels\n",
        "predicted_labels = ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC']\n",
        "true_labels = ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC']\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebdf53c",
      "metadata": {
        "id": "0ebdf53c"
      },
      "source": [
        "### N-gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5abc9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac5abc9e",
        "outputId": "d087cf51-1912-4518-ddc3-221e47176bd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Unigrams (N=1): [('asian',), ('exporters',), ('fear',), ('damage',), ('from',)]\n",
            "Example Bigrams (N=2): [('asian', 'exporters'), ('exporters', 'fear'), ('fear', 'damage'), ('damage', 'from'), ('from', 'u.s.-japan')]\n",
            "Example Trigrams (N=3): [('asian', 'exporters', 'fear'), ('exporters', 'fear', 'damage'), ('fear', 'damage', 'from'), ('damage', 'from', 'u.s.-japan'), ('from', 'u.s.-japan', 'rift')]\n",
            "Example 4-grams (N=4): [('asian', 'exporters', 'fear', 'damage'), ('exporters', 'fear', 'damage', 'from'), ('fear', 'damage', 'from', 'u.s.-japan'), ('damage', 'from', 'u.s.-japan', 'rift'), ('from', 'u.s.-japan', 'rift', 'mounting')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load and tokenize text data (e.g., from the Reuters corpus)\n",
        "reuters_text = reuters.raw()\n",
        "tokens = word_tokenize(reuters_text.lower())\n",
        "\n",
        "# Define function to generate N-grams of given order\n",
        "def generate_ngrams(token_list, n):\n",
        "    return list(ngrams(token_list, n))\n",
        "\n",
        "# Example usage:\n",
        "# Unigrams (N=1)\n",
        "unigrams = generate_ngrams(tokens, 1)\n",
        "\n",
        "# Bigrams (N=2)\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "\n",
        "# Trigrams (N=3)\n",
        "trigrams = generate_ngrams(tokens, 3)\n",
        "\n",
        "# N-grams (N>3)\n",
        "n = 4\n",
        "ngrams = generate_ngrams(tokens, n)\n",
        "\n",
        "# Print example outputs\n",
        "print(\"Example Unigrams (N=1):\", unigrams[:5])\n",
        "print(\"Example Bigrams (N=2):\", bigrams[:5])\n",
        "print(\"Example Trigrams (N=3):\", trigrams[:5])\n",
        "print(f\"Example {n}-grams (N={n}):\", ngrams[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s1yDNGCSkkBP",
      "metadata": {
        "id": "s1yDNGCSkkBP"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21baa7a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21baa7a9",
        "outputId": "cf836c4e-bcf7-4543-c219-814a8dbdb661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector representation of 'finance': [ 0.00555959 -0.13418159 -0.07054096  0.2715022  -0.46067762 -0.57456183\n",
            "  0.36883485  0.07910629 -0.49893373 -0.45637354 -0.10879581  0.01425171\n",
            " -0.12155091  0.49947003 -0.20474729 -0.02343808  0.20650081  0.07367455\n",
            " -0.01277893 -0.10894115 -0.17248821 -0.03425761  0.31837705 -0.3576945\n",
            " -0.57092845 -0.21395148  0.19104598 -0.21550281 -0.6066371   0.08843739\n",
            " -0.05022264  0.38166136  0.37446606 -0.359757   -0.22131883  0.11892209\n",
            " -0.03559355 -0.42578197 -1.0066364   0.2839204   0.23510082 -0.23057513\n",
            " -0.3144356   0.25781754  0.7809881  -0.05510634  0.7119717  -0.09271256\n",
            "  0.00404422  0.38225052  0.03035207 -0.25748873 -0.0915408  -0.14833567\n",
            " -0.29154584  0.25353318 -0.04460439  0.2662932  -0.16877227 -0.12203691\n",
            "  0.26362306  0.1239363   0.5448731  -0.06204756 -0.4014762   0.59350646\n",
            " -0.0728      0.09463657  0.34893999  0.5589005   0.17783362  0.22128652\n",
            " -0.14458849  0.05690804  0.4040991  -0.09201758 -0.17391795  0.15579928\n",
            " -0.01704998  0.29850703 -0.4886291   0.23450014  0.11355773  0.31036708\n",
            "  0.2798942   0.20305444 -0.08241291  0.0854152   0.28603467 -0.0222973\n",
            " -0.32133728  0.1789765   1.0509369  -0.36064497  0.64802295  0.4207522\n",
            "  0.5092944  -0.04728353 -0.19372149 -0.4943172 ]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "# Load and tokenize text data (e.g., from the Reuters corpus)\n",
        "reuters_text = reuters.raw()\n",
        "sentences = [word_tokenize(word.lower()) for word in sent_tokenize(reuters_text)]\n",
        "\n",
        "# Train Word2Vec model using Skip-gram architecture\n",
        "word2vec_model = Word2Vec(sentences, sg=1, vector_size=100, window=5, min_count=5)\n",
        "\n",
        "# Get vector representation of a word\n",
        "vector = word2vec_model.wv['finance']\n",
        "print(\"Vector representation of 'finance':\", vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2axc62qllYG0",
      "metadata": {
        "id": "2axc62qllYG0"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P3aIbHOJlXYF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3aIbHOJlXYF",
        "outputId": "d694a5fe-aad4-4aa0-c566-98706b7ef849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: king, Vector Shape: (25,)\n",
            "First 10 elements: [-0.74501 -0.11992  0.37329  0.36847 -0.4472  -0.2288   0.70118  0.82872\n",
            "  0.39486 -0.58347]\n",
            "Similarity between 'king' and 'queen': 0.9202\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the path to your downloaded GloVe file (adjust filename and dimensionality)\n",
        "glove_file = \"/content/glove.twitter.27B.25d.txt\"\n",
        "\n",
        "# Create a dictionary to store word vectors\n",
        "word_vectors = {}\n",
        "\n",
        "# Read the GloVe file and populate the dictionary\n",
        "with open(glove_file, encoding=\"utf8\") as file:\n",
        "  for line in file:\n",
        "    line_split = line.split()\n",
        "    word = line_split[0]\n",
        "    vector = np.array([float(val) for val in line_split[1:]])\n",
        "    word_vectors[word] = vector\n",
        "\n",
        "# Example usage: Find the vector representation of a word\n",
        "word = \"king\"\n",
        "if word in word_vectors:\n",
        "  word_vector = word_vectors[word]\n",
        "  print(f\"Word: {word}, Vector Shape: {word_vector.shape}\")\n",
        "  # Access individual vector components (example: first 10 elements)\n",
        "  print(f\"First 10 elements: {word_vector[:10]}\")\n",
        "else:\n",
        "  print(f\"Word '{word}' not found in vocabulary\")\n",
        "\n",
        "# Example usage: Calculate word similarity using cosine similarity\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "if word1 in word_vectors and word2 in word_vectors:\n",
        "  vector1 = word_vectors[word1]\n",
        "  vector2 = word_vectors[word2]\n",
        "  similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "  print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
        "else:\n",
        "  print(f\"At least one word not found in vocabulary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HFSz7LRwdtiU",
      "metadata": {
        "id": "HFSz7LRwdtiU"
      },
      "source": [
        " ### TF-IDF NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ZbRlm1duJR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18ZbRlm1duJR",
        "outputId": "1f3e58d3-86d6-46ab-e414-4daf6e6b4c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "   cat: 0.3410\n",
            "   mat: 0.4484\n",
            "   on: 0.4484\n",
            "   sat: 0.4484\n",
            "   the: 0.5297\n",
            "Document 2:\n",
            "   dog: 0.3410\n",
            "   fence: 0.4484\n",
            "   jumped: 0.4484\n",
            "   over: 0.4484\n",
            "   the: 0.5297\n",
            "Document 3:\n",
            "   and: 0.3907\n",
            "   are: 0.3907\n",
            "   best: 0.3907\n",
            "   cat: 0.2971\n",
            "   dog: 0.2971\n",
            "   friends: 0.3907\n",
            "   the: 0.4615\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus of documents\n",
        "corpus = [\n",
        "    'The cat sat on the mat.',\n",
        "    'The dog jumped over the fence.',\n",
        "    'The cat and the dog are best friends.'\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit-transform the corpus to calculate TF-IDF scores\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names (words) from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF-IDF scores for each word in the corpus\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j, word in enumerate(feature_names):\n",
        "        tfidf_score = tfidf_matrix[i, j]\n",
        "        if tfidf_score > 0:\n",
        "            print(f\"   {word}: {tfidf_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KqYfszX-_cGS",
      "metadata": {
        "id": "KqYfszX-_cGS"
      },
      "source": [
        "### Syntax and syntactic analysis in NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ix7ydnilXbJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ix7ydnilXbJ",
        "outputId": "344ecc51-aad6-44ac-b778-34724d2afaca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The det cat NOUN []\n",
            "cat nsubj sat VERB [The]\n",
            "sat ROOT sat VERB [cat, on, .]\n",
            "on prep sat VERB [mat]\n",
            "the det mat NOUN []\n",
            "mat pobj on ADP [the]\n",
            ". punct sat VERB []\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text for syntactic analysis\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Perform syntactic analysis\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print dependency parse tree\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rc98X8Tg4ae",
      "metadata": {
        "id": "_Rc98X8Tg4ae"
      },
      "source": [
        "### Dependency parsing vs. constituency parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nFP1thlVg4hS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFP1thlVg4hS",
        "outputId": "1042fdb5-ff26-4635-c243-d9750d6e6606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependency Parsing:\n",
            "The det cat\n",
            "cat nsubj sat\n",
            "sat ROOT sat\n",
            "on prep sat\n",
            "the det mat\n",
            "mat pobj on\n",
            ". punct sat\n",
            "\n",
            "Constituency Parsing:\n",
            "The cat\n",
            "the mat\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text for parsing\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Perform dependency parsing\n",
        "doc_dep = nlp(text)\n",
        "print(\"Dependency Parsing:\")\n",
        "for token in doc_dep:\n",
        "    print(token.text, token.dep_, token.head.text)\n",
        "\n",
        "# Perform constituency parsing (using spaCy's noun chunks)\n",
        "doc_const = nlp(text)\n",
        "print(\"\\nConstituency Parsing:\")\n",
        "for chunk in doc_const.noun_chunks:\n",
        "    print(chunk.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Osn8x2RGkFTz",
      "metadata": {
        "id": "Osn8x2RGkFTz"
      },
      "source": [
        "### Word embeddings and semantic similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvcMcuc3kFfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvcMcuc3kFfb",
        "outputId": "a960547a-3e20-4a83-e9c2-fc41caccda31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words similar to 'car':\n",
            "vehicle: 0.7821096181869507\n",
            "cars: 0.7423831224441528\n",
            "SUV: 0.7160962224006653\n",
            "minivan: 0.6907036900520325\n",
            "truck: 0.6735789775848389\n",
            "Car: 0.6677608489990234\n",
            "Ford_Focus: 0.667320191860199\n",
            "Honda_Civic: 0.6626849174499512\n",
            "Jeep: 0.651133120059967\n",
            "pickup_truck: 0.6441438794136047\n",
            "===================================\n",
            "Semantic similarity between 'king' and 'man': 0.22942672669887543\n",
            "Semantic similarity between 'king' and 'woman': 0.1284797340631485\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Example usage: Finding similar words\n",
        "word = \"car\"\n",
        "similar_words = model.most_similar(word)\n",
        "print(f\"Words similar to '{word}':\")\n",
        "for similar_word, similarity_score in similar_words:\n",
        "    print(f\"{similar_word}: {similarity_score}\")\n",
        "\n",
        "print(\"=\" * 35)\n",
        "# Example usage: Word vector arithmetic\n",
        "word1 = \"king\"\n",
        "word2 = \"man\"\n",
        "word3 = \"woman\"\n",
        "\n",
        "# Compute semantic similarity\n",
        "similarity1_2 = model.similarity(word1, word2)\n",
        "similarity1_3 = model.similarity(word1, word3)\n",
        "\n",
        "# Print results\n",
        "print(f\"Semantic similarity between '{word1}' and '{word2}': {similarity1_2}\")\n",
        "print(f\"Semantic similarity between '{word1}' and '{word3}': {similarity1_3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UXaaLbnSuu2l",
      "metadata": {
        "id": "UXaaLbnSuu2l"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdd9c99",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install spacy ==3.7.5\n",
        "! pip install spacytextblob ==5.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ziaAtcnknajD",
      "metadata": {
        "id": "ziaAtcnknajD"
      },
      "outputs": [],
      "source": [
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zliHka4ioJly",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zliHka4ioJly",
        "outputId": "94561f66-aa23-4113-fd7b-09b4629a7b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text: good product\n",
            "Polarity: 0.7\n",
            "Subjectivity: 0.6000000000000001\n",
            "===================================\n",
            "text: bad product\n",
            "Polarity: -0.6999999999999998\n",
            "Subjectivity: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Add the TextBlob extension to the pipeline\n",
        "nlp.add_pipe('spacytextblob')\n",
        "\n",
        "\n",
        "text = \"good product\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the sentiment scores\n",
        "print('text:', text)\n",
        "print('Polarity:', doc._.blob.polarity)\n",
        "print('Subjectivity:', doc._.blob.subjectivity)\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Process some text\n",
        "text = \"bad product\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the sentiment scores\n",
        "print('text:', text)\n",
        "print('Polarity:', doc._.blob.polarity)\n",
        "print('Subjectivity:', doc._.blob.subjectivity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVcyaJpWogWQ",
      "metadata": {
        "id": "zVcyaJpWogWQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
