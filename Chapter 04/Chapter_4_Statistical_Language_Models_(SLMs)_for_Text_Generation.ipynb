{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Language Models"
      ],
      "metadata": {
        "id": "5ObPBGv2MSWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"the cat sat on the mat\", \"the dog ate the bone\"]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = [word for sentence in corpus for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "# Function to build an N-gram language model\n",
        "def build_ngram_model(corpus, n):\n",
        "    ngrams = defaultdict(list)\n",
        "    for i in range(len(corpus) - n + 1):\n",
        "        context = tuple(corpus[i:i + n - 1])\n",
        "        next_word = corpus[i + n - 1]\n",
        "        ngrams[context].append(next_word)\n",
        "    return ngrams\n",
        "\n",
        "# Example usage\n",
        "ngram_model = build_ngram_model(tokens, 2)\n",
        "print(ngram_model)\n"
      ],
      "metadata": {
        "id": "RUQAocGnMRqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d72ca6b-c4bf-401d-8eeb-15558c09b1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {('the',): ['cat', 'mat', 'dog', 'bone'], ('cat',): ['sat'], ('sat',): ['on'], ('on',): ['the'], ('mat',): ['the'], ('dog',): ['ate'], ('ate',): ['the']})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to generate text using an N-gram language model\n",
        "def generate_text(model, n, num_words):\n",
        "    current_context = random.choice(list(model.keys()))\n",
        "    generated_text = list(current_context)\n",
        "    for _ in range(num_words):\n",
        "        next_word = random.choice(model[current_context])\n",
        "        generated_text.append(next_word)\n",
        "        current_context = tuple(generated_text[-(n - 1):])\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Example usage\n",
        "generated_text = generate_text(ngram_model, 2, 10)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndRgMVYPpjTe",
        "outputId": "68cccc49-d91b-40bc-be1a-76a72e2f4684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat sat on the mat the cat sat on the bone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov chain simulation"
      ],
      "metadata": {
        "id": "vwuHZRiOA8Lc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMqE11qlMOvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d23d917-1cbe-46c7-9010-be87275f4f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current State: 1\n",
            "Current State: 0\n",
            "Current State: 0\n",
            "Current State: 0\n",
            "Current State: 1\n",
            "Current State: 0\n",
            "Current State: 1\n",
            "Current State: 1\n",
            "Current State: 1\n",
            "Current State: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transition matrix for a simple 2-state Markov chain\n",
        "transition_matrix = np.array([[0.7, 0.3],   # Probability of going from state 0 to state 0 or state 1\n",
        "                              [0.4, 0.6]])  # Probability of going from state 1 to state 0 or state 1\n",
        "\n",
        "# Initial state probabilities\n",
        "initial_state = np.array([0.5, 0.5])  # Equal probability of starting in state 0 or state 1\n",
        "\n",
        "# Simulate Markov Chain\n",
        "num_steps = 10\n",
        "current_state = np.random.choice([0, 1], p=initial_state)  # Start from initial state\n",
        "for _ in range(num_steps):\n",
        "    print(\"Current State:\", current_state)\n",
        "    current_state = np.random.choice([0, 1], p=transition_matrix[current_state])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Chain text generator"
      ],
      "metadata": {
        "id": "bCInmnWjBBhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install markovify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TheX3Pxzm5U6",
        "outputId": "d0010078-3468-4c0b-d940-371dedb46ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18608 sha256=4b1d3a5361ef192fa98481bd95a8a8f687fd2137f75c9122d8f86594988b6b5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/8c/c5/41413e24c484f883a100c63ca7b3b0362b7c6f6eb6d7c9cc7f\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markovify\n",
        "\n",
        "# open the text file that you wish to train the model with\n",
        "with open(\"/content/poetry.txt\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Build the model.\n",
        "text_model = markovify.Text(text)\n",
        "\n",
        "# Print five randomly-generated sentences\n",
        "for i in range(5):\n",
        "    print(text_model.make_sentence())\n",
        "\n",
        "print(\"=\"*55)\n",
        "# Print three randomly-generated sentences of no more than 280 characters\n",
        "for i in range(3):\n",
        "    print(text_model.make_short_sentence(280))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUJIaBKTnaU-",
        "outputId": "29212e1b-5f9e-4ed0-fcc0-ef0bdb1e5ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sing a song full of the stairs, counting each drop of pearl made with water— blue on the thumb of our silent tears, Thou who hast by Thy might, Led us into the mountains of Japan.\n",
            "It belongs to the masquerade.\n",
            "The blonde has her eyes bestowed upon her warrior!\n",
            "Her nights were dreams of the old days I have this trowel, these overalls, this ridiculous hat now.\n",
            "Ere sleep comes down to soothe the weary eyes, How all the cark of time had flown, And I was crazy try’n’a dream us up a future even if I were descending into a new desire.\n",
            "=======================================================\n",
            "Like petals of a myth that Hard Rock did nothing.\n",
            "You, Lord, as you have left is that middle finger around your God-given right to be listening anyway.\n",
            "What kind of clemency, against the tall forest of sharp pines, the morning air fulvousWith a metallic syncopation,A key to a grim quietude.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naïve Bayes classifier for text classification"
      ],
      "metadata": {
        "id": "FhEJ7q7iBHFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample text data for training\n",
        "train_text = [\"I love this movie\", \"This movie is great\", \"I dislike this movie\", \"This movie is awful\"]\n",
        "\n",
        "# Corresponding labels for each text sample\n",
        "train_labels = np.array([1, 1, 0, 0])  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_text, train_labels)\n",
        "\n",
        "# Sample test data for prediction\n",
        "test_text = [\"I love this product\", \"This product is terrible\"]\n",
        "\n",
        "# Predict sentiment for test data\n",
        "predicted_labels = model.predict(test_text)\n",
        "\n",
        "# Output the predicted labels\n",
        "for text, label in zip(test_text, predicted_labels):\n",
        "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
        "    print(f\"Text: {text} -- Predicted Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "id": "rkkDXOIvMRtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d443ced7-5b51-4fbb-c047-bcb709046b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product -- Predicted Sentiment: positive\n",
            "Text: This product is terrible -- Predicted Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### probability distributions in an N-gram model"
      ],
      "metadata": {
        "id": "PiA1pCHJBP2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"the cat sat on the mat\", \"the dog ate the bone\"]\n",
        "\n",
        "# Create unigram probability distribution\n",
        "unigram_counts = defaultdict(int)\n",
        "total_words = 0\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "        unigram_counts[word] += 1\n",
        "        total_words += 1\n",
        "\n",
        "unigram_probabilities = {word: count / total_words for word, count in unigram_counts.items()}\n",
        "print(\"Unigram Probabilities:\", unigram_probabilities)\n"
      ],
      "metadata": {
        "id": "xHuiBzNTMRv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ec83c4-1a9a-4a36-c814-3720f0975029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities: {'the': 0.36363636363636365, 'cat': 0.09090909090909091, 'sat': 0.09090909090909091, 'on': 0.09090909090909091, 'mat': 0.09090909090909091, 'dog': 0.09090909090909091, 'ate': 0.09090909090909091, 'bone': 0.09090909090909091}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  N-gram Language Models"
      ],
      "metadata": {
        "id": "xnpEZ-uPBVsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"the cat sat on the mat\", \"the dog ate the bone\"]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = [word for sentence in corpus for word in sentence.split()]\n",
        "\n",
        "# Calculate unigram counts\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Vocabulary size\n",
        "V = len(set(tokens))\n",
        "\n",
        "# Laplace Smoothing\n",
        "def laplace_smoothing(word, k=1):\n",
        "    return (unigram_counts[word] + k) / (len(tokens) + k * V)\n",
        "\n",
        "# Additive Smoothing\n",
        "def additive_smoothing(word, alpha=0.5):\n",
        "    return (unigram_counts[word] + alpha) / (len(tokens) + alpha * V)\n",
        "\n",
        "# Good-Turing Smoothing\n",
        "def good_turing_smoothing(word):\n",
        "    # Count of counts\n",
        "    counts_of_counts = Counter(unigram_counts.values())\n",
        "    c = unigram_counts[word]\n",
        "    N1 = counts_of_counts[1]  # Count of unigrams that occur once\n",
        "    if c + 1 in counts_of_counts:\n",
        "        Nc_plus_1 = counts_of_counts[c + 1]\n",
        "    else:\n",
        "        Nc_plus_1 = 0\n",
        "    return (c + 1) * Nc_plus_1 / N1 / len(tokens)\n",
        "\n",
        "# Example usage\n",
        "word = \"cat\"\n",
        "print(\"Laplace Smoothing:\", laplace_smoothing(word))\n",
        "print(\"Additive Smoothing:\", additive_smoothing(word))\n",
        "print(\"Good-Turing Smoothing:\", good_turing_smoothing(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdctzZw9deQB",
        "outputId": "a78e24a7-eb5a-4fea-907e-05da545552b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothing: 0.10526315789473684\n",
            "Additive Smoothing: 0.1\n",
            "Good-Turing Smoothing: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruning technique\n"
      ],
      "metadata": {
        "id": "OR4V9iub3ULX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"the cat sat on the mat\", \"the dog ate the bone\"]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = [word.lower() for sentence in corpus for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "# Function to build a unigram language model with pruning\n",
        "def build_pruned_unigram_model(corpus, threshold):\n",
        "    unigram_counts = defaultdict(int)\n",
        "    total_words = 0\n",
        "    for word in corpus:\n",
        "        unigram_counts[word] += 1\n",
        "        total_words += 1\n",
        "    pruned_unigram_model = {word: count / total_words for word, count in unigram_counts.items() if count >= threshold}\n",
        "    return pruned_unigram_model\n",
        "\n",
        "# Build the pruned unigram language model\n",
        "threshold = 2  # Example threshold for pruning\n",
        "pruned_unigram_model = build_pruned_unigram_model(tokens, threshold)\n",
        "\n",
        "print(\"Pruned Unigram Model:\")\n",
        "print(pruned_unigram_model)\n"
      ],
      "metadata": {
        "id": "7RYBuYgV3TPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}